{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sivtempfit generative model development\n",
    "\n",
    "**Ruffin Evans**\n",
    "\n",
    "2016-04-10\n",
    "\n",
    "Last Update: 2016-05-05\n",
    "\n",
    "This notebook develops the generative model that will eventually be used to fit the spectra data in this package.\n",
    "\n",
    "I'll do this in two steps.\n",
    "\n",
    "First, I'll write down the generative model. The end result of this section will be to produce a spectrum that \"looks like\" the spectra produced experimentally depending on the parameters.\n",
    "\n",
    "Next, given this generative model, I will write down the likelihood function which expresses how well the data agrees with the model (literally, the likelihood that the data is drawn from the model.) The main part that is hard about this is that I need to make a statement about the uncertanties in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# makes plots display inline in notebook interface:\n",
    "%matplotlib inline \n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"matplotlib\")\n",
    "import emcee\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats # Use for KDE in the end\n",
    "import scipy.optimize as opt # Use for minimizing KDE\n",
    "import scipy as sp\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Model\n",
    "\n",
    "The model that I'll start with is fairly simple. It contains two peaks: one for the laser line, and one for the SiV itself.\n",
    "\n",
    "Some preliminary fitting suggests that both peaks are very well fit by a Lorentzian and not by a Gaussian. Physically, it's not clear what the situation should be. For a single SiV, the spectrum should be Lorentzian, but we generally measure an ensemble of SiVs with enough of an inhomogeneous distribution that there could be a Gaussian component.\n",
    "\n",
    "In the future, therefore, I may upgrade this model to fit a [Voigt profile](http://scipython.com/book/chapter-8-scipy/examples/the-voigt-profile/) to these lines instead. But this is possibly computationally more expensive (both in terms of evaluating the function and in terms of the added parameters) so let's just stick with the two-lorentzian model for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### A word about resolution\n",
    "\n",
    "Vinny suggested that I include in my model some \"blur\" from the spectrometer itself. This effects the two peaks in the spectrum in two separate ways.\n",
    "\n",
    "The laser line is intrinsically much narrower ($\\lesssim 100 \\mathrm{MHz}$) than the resolution of the spectrometer ($\\sim10\\mathrm{GHz}$). Hence, the laser line is only limited by the resolution of the spectrometer, and blurring this line would be accounting for this effect twice.\n",
    "\n",
    "The SiV line is intrinsically much broader ($\\sim2\\mathrm{THz}$) than the resolution of the spectrometer, so the effect of the spectrometer resolution will be minimal.\n",
    "\n",
    "Finally, if both lines are indeed lorentzians, then that means the transfer function of the spectrometer is a lorentzian. Since the siv line is lorentzian by assumption, and the convolution of two lorentzians is a lorentzian, fitting a single lorentzian is actually general in this case.\n",
    "\n",
    "Motivated by these considerations, I will not introduce a specific blurring from the spectrometer for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction of the model\n",
    "\n",
    "The model here has two Lorentzian peaks plus a constant background. The peaks have arbitrary amplitude and width (although we have strong priors on the width on the narrower (laser) peak).\n",
    "\n",
    "We'll parameterize the center positions of the peak with two variables: $C_2$ and $C_2-C_1$. This choice is motivated by the desire to use the laser peak with center $C_2$ as a calibration, so $C_1-C_2+\\lambda_2$ is the actual center position of peak 1 independent of the wavelength calibration of the instrument since $\\lambda_2$ is known exactly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define a standard Lorentzian. This could be done by using scipy.stats.cauchy.pdf , but I'm not sure if that is efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lorentz(x, center, width):\n",
    "    \"\"\"\n",
    "    A lorentzian function with peak position 'center' and FWHM 'width'.\n",
    "    \"\"\"\n",
    "    return (1/np.pi)*(width/2)/((x-center)**2+(width/2)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def two_peak_model(x, amp1, amp2, center_offset, center2, width1, width2, background):\n",
    "    return background + amp1*lorentz(x, center_offset + center2, width1) + amp2*lorentz(x, center2, width2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we expect the spectra to look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x275f22bd278>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFVCAYAAADVDycqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x275f21e4710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xrange = np.arange(0,100,0.1)\n",
    "plt.plot(xrange, two_peak_model(xrange, 500, 10, -30, 70, 20, 0.1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood\n",
    "\n",
    "### Introduction\n",
    "\n",
    "As noted above, this requires us to make statements about the noise in our system. We have characterized our system a bit and the noise seems to be primarily limited by shot noise. \n",
    "\n",
    "There are some subleties here, however. The shot noise is only on the photons that are actually detected, whereas there is also a readout noise on the detector of \"three electrons\" (according to the datasheet) per bin. Absent any other information, I assume this is gaussian noise. Moreover, the detector itself contributes a more or less fixed background (of around 1000 counts) on top of this noise. So, we'll include an additional term in the likelihood that includes this fixed CCD readout background plus associated noise.\n",
    "\n",
    "The likelihood is then the product of two terms: one is a Gaussian likelihood for the CCD background, and one is a Poisson likelihood for the actual photon signal. In each of these two terms, we subtract the contribution to the signal from the other term before we calculate that contribution to the likelihood. In other words, for the CCD background, we take the signal, subtract what we think is coming from the light entering the instrument --- which is the two_peak_model --- and then consider Gaussian noise on that residual bit. For the poisson noise, we subtract the CCD offset from the signal before we fit the poisson noise.\n",
    "\n",
    "I am not sure if this is the correct thing to do, because it means that if there is gaussian noise on the CCD, it still reduces the likelihood for the poisson contribution because we are subtracting only the background and not any noise. I will try to figure out if this is legit.\n",
    "\n",
    "### Note on parameterization\n",
    "\n",
    "In the future, we will test a model for the temperature dependence of $C_1$ which we expect to be approximately linear over a suitable range. Anticipating this, we will write $C_1 = (T-T_0)\\times m+\\lambda_0$ where $m = \\frac{d\\lambda}{dT}$ and $T_0$ and $\\lambda_0$ are the offsets for the linear scaling. We can combine these two offsets into a single dummy variable $C_0$.\n",
    "\n",
    "Note that this linear scaling is nonphysical: it is just an approximation that holds fairly well over a small temperature range. We'll have to change this formulation slightly to test higher order models for the temperature scaling.\n",
    "\n",
    "Here, we will also switch to the (for now, redundant) parameterization in terms of $T$ and $m$, plus an offset $C_0$. For fitting a single spectrum, $T$ can be set to zero and the fit will be in terms of $C_0$ only.\n",
    "\n",
    "\n",
    "### Formal construction of likelihood\n",
    "\n",
    "(For this section, I found the following paper useful: http://epubs.siam.org/doi/abs/10.1137/15M1014395?journalCode=sjisbi)\n",
    "\n",
    "For each data point $y_i$ we have the prediction of our model $z_i=m_i(\\theta)$ plus two sources of error $f_i$ and $g_i$ corresponding to Poisson and Gaussian noise respectively.\n",
    "\n",
    "$$y_i = m_i(\\theta) + f_i + g_i$$\n",
    "\n",
    "We assume that these two sources of error (photon shot noise and electron readout noise) are uncorrelated. Then we can write down the joint distribution:\n",
    "\n",
    "$$e_i = f_i + g_i$$\n",
    "\n",
    "which is just the convolution of the two noise sources. Once we have $e_i$, the formulation we have learned in class can be applied. (Alternatively, we can go through the integral formulation that Gregory does with these two sources of error -- there will be an extra integral involved and the convlution will arise organically.) Following this logic, the likelihood to detect $y_i$ photons is the convolution of the likelihood terms from the two error processes:\n",
    "\n",
    "$$\\mathcal{L}(y_i) \\equiv p(y_i \\mid M, \\theta, I) = \\sum_{n=0}^\\infty\\frac{\\mu_i^n}{n!}e^{-\\mu_i}\\cdot \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{((y_i-n)-b)^2}{2\\sigma^2}}$$\n",
    "\n",
    "Let's unpack this. We have a (discrete) convolution of two terms, which intuitively expresses the idea that we are detecting $y_i$ photons but that this signal could be divided different ways between the two sources of noise.\n",
    "\n",
    "The first term arises from considering the Poisson distribution with mean $\\mu_i$. Here, $\\mu_i$ is the value predicted from the model above and accounts only for the photons hitting the detector (which can include some background).\n",
    "\n",
    "The second term arises from considering the Gaussian distribution with mean $b$ and standard deviation $\\sigma$. This term has nothing to do with the above model (in that sense, the above model is incomplete or at least underparameterized) and arises from the CCD background noise which is Gaussian with some mean and standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical interlude\n",
    "\n",
    "To calculate the likelihood, we need to perform a convolution (which amounts to performing a sum) for each data point. This gives the likelihood of a single data point. Then we need to multiply together the likelihood of all of these data points and take the log.\n",
    "\n",
    "We can do this in a `for` loop, but in python it's better if we can use numpy to do this in a fully vectorized way. How do we do that?\n",
    "\n",
    "Well, we have three lists: the list of predictions for the model, the list of the actual data point values, and the photon numbers $n$ over which to perform the convolution. Two of these lists have the same dimensions and have corresponding values, but the photon number list is a different animal -- its values do not \"match up\" with the values in the other two lists. Rather, we want to do a sort of outer product where we perform the convolution for each data point in the data and model lists.\n",
    "\n",
    "This requires telling `numpy` how to do the broadcasting. Here is a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.   0.5  1. ]\n",
      "[ 0.    0.25  0.5   0.75  1.  ]\n",
      "[[ 0.  ]\n",
      " [ 0.25]\n",
      " [ 0.5 ]\n",
      " [ 0.75]\n",
      " [ 1.  ]]\n",
      "[[ 1.          1.          1.        ]\n",
      " [ 0.          0.84089642  1.        ]\n",
      " [ 0.          0.70710678  1.        ]\n",
      " [ 0.          0.59460356  1.        ]\n",
      " [ 0.          0.5         1.        ]]\n",
      "(5, 3)\n",
      "[ 1.          3.64260675  5.        ]\n",
      "[[        nan         inf         inf]\n",
      " [ 0.          1.68179283  4.        ]\n",
      " [ 0.          0.70710678  2.        ]\n",
      " [ 0.          0.39640237  1.33333333]\n",
      " [ 0.          0.25        1.        ]]\n",
      "1.681792830507429\n"
     ]
    }
   ],
   "source": [
    "ydata = np.arange(0,1.1,0.5)\n",
    "conv_range = np.arange(0, 1.1, 0.25)\n",
    "# Can do the outer product like this\n",
    "new_data = (ydata**conv_range[:,np.newaxis])\n",
    "print(ydata)\n",
    "print(conv_range)\n",
    "print(conv_range[:,np.newaxis])\n",
    "print(new_data)\n",
    "print(new_data.shape)\n",
    "# Zero sums along columns, 1 sums along rows\n",
    "# Here, doing a*b[:,np.newaxis] gives a table with dims (len(b), len(a)) that has [b_1*a, b_2*a, b_3*a]\n",
    "# So summing along axis 0 is the correct thing to do if we want to do convolution and b is the set of things to convolve.\n",
    "# Then we have a list of likelihoods, which we can np.product and then log\n",
    "print(np.sum(new_data, 0))\n",
    "# Seems like you need to do np.newaxis thing after each operation with the convolution variable\n",
    "# But not for the y data variable; the axes match up and that can be broadcast.\n",
    "print((ydata**conv_range[:,np.newaxis])/(conv_range[:,np.newaxis])*ydata)\n",
    "# Element 2, 2 should be:\n",
    "print(0.5**0.25/0.25*0.5)                                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when we want to do the convolution, each point should have the convolution done over a different range because it has a different number of photons. So what we really want is a different convolution list for each y value. Should just be $n=(y-b)\\pm5\\sigma$ (but at least zero) since Gaussian has essentially negligible support elsewhere. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.  -4.5 -4. ]\n",
      " [-4.  -3.5 -3. ]\n",
      " [-3.  -2.5 -2. ]\n",
      " [-2.  -1.5 -1. ]\n",
      " [-1.  -0.5  0. ]\n",
      " [ 0.   0.5  1. ]\n",
      " [ 1.   1.5  2. ]\n",
      " [ 2.   2.5  3. ]\n",
      " [ 3.   3.5  4. ]\n",
      " [ 4.   4.5  5. ]]\n",
      "[ 0.   0.5  1. ]\n",
      "[[             inf   4.32293011e-01   4.16666667e-02]\n",
      " [             inf   9.72659274e-01   1.66666667e-01]\n",
      " [             inf   1.70215373e+00   5.00000000e-01]\n",
      " [             inf   2.12769216e+00   1.00000000e+00]\n",
      " [             inf   1.59576912e+00   1.00000000e+00]\n",
      " [  1.00000000e+00   7.97884561e-01   1.00000000e+00]\n",
      " [  0.00000000e+00   2.65961520e-01   5.00000000e-01]\n",
      " [  0.00000000e+00   5.31923041e-02   1.66666667e-01]\n",
      " [  0.00000000e+00   7.59890058e-03   4.16666667e-02]\n",
      " [  0.00000000e+00   8.44322287e-04   8.33333333e-03]]\n",
      "(10, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([        inf,  7.95604891,  4.425     ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ydata = np.arange(0,1.1,0.5)\n",
    "conv_range = np.arange(-5, 5)[:,np.newaxis]\n",
    "# Each element in this list is a list of values over which to do the convolution.\n",
    "conv_matrix = (ydata+conv_range)\n",
    "print(conv_matrix)\n",
    "# Now we can perform manipulations like so:\n",
    "new_data = (ydata**conv_matrix)/(sp.misc.factorial(abs(conv_matrix)))\n",
    "print(ydata)\n",
    "print(new_data)\n",
    "print(new_data.shape)\n",
    "# To get the convolved data out, just do sum along first axis:\n",
    "np.sum(new_data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to be careful that we don't divide by zero, which will happen if we pass a negative number to the factorial function. To do this, replace negative numbers by NaN, and then set all NaNs to zero. Can do this with masking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.  -4.5 -4. ]\n",
      " [-4.  -3.5 -3. ]\n",
      " [-3.  -2.5 -2. ]\n",
      " [-2.  -1.5 -1. ]\n",
      " [-1.  -0.5  0. ]\n",
      " [ 0.   0.5  1. ]\n",
      " [ 1.   1.5  2. ]\n",
      " [ 2.   2.5  3. ]\n",
      " [ 3.   3.5  4. ]\n",
      " [ 4.   4.5  5. ]]\n",
      "[-5.  -4.5 -4.  -4.  -3.5 -3.  -3.  -2.5 -2.  -2.  -1.5 -1.  -1.  -0.5]\n",
      "[[ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  nan]\n",
      " [ nan  nan  0. ]\n",
      " [ 0.   0.5  1. ]\n",
      " [ 1.   1.5  2. ]\n",
      " [ 2.   2.5  3. ]\n",
      " [ 3.   3.5  4. ]\n",
      " [ 4.   4.5  5. ]]\n"
     ]
    }
   ],
   "source": [
    "conv_matrix = (ydata+conv_range)\n",
    "print(conv_matrix)\n",
    "print(conv_matrix[conv_matrix<0])\n",
    "conv_matrix[conv_matrix<0] = np.nan\n",
    "print(conv_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full specification of likelihood\n",
    "\n",
    "We can now construct the likelihood function. I will simply reference the latest version from the package, because it is the most up-to-date and best documented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sivtempfit.model\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['def two_peak_log_likelihood(x, y, amp1, amp2, T, m, C0, center2,\\n',\n",
       "  '                            width1, width2, light_background,\\n',\n",
       "  '                            ccd_background, ccd_stdev,\\n',\n",
       "  '                            conv_range = -1, debug = False,\\n',\n",
       "  '                            test_norm = False, safe = False,\\n',\n",
       "  '                            gaussian_approx = False):\\n',\n",
       "  '    \"\"\"\\n',\n",
       "  '    Returns the log-likelihood calculated for the two-peak + CCD noise model.\\n',\n",
       "  '    See also: two_peak_model\\n',\n",
       "  '    Parameters:\\n',\n",
       "  '    -----------\\n',\n",
       "  '    x : wavelength or x-axis value\\n',\n",
       "  '    y : corresponding observed data value\\n',\n",
       "  '    amp1 : amplitude of the broad SiV peak in the spectrum\\n',\n",
       "  '    amp2 : amplitude of the narrow calibration peak in the spectrum\\n',\n",
       "  '    T : The temperature of the sample\\n',\n",
       "  '    m : The linear scaling of the SiV peak position with temperature\\n',\n",
       "  '    C0 : The offset in the above linear scaling\\n',\n",
       "  '    center2 : The position of the calibration line\\n',\n",
       "  '    width1 : The width (FWHM) of the SiV line\\n',\n",
       "  '    width2 : The width (FWHM) of the calibration line\\n',\n",
       "  '    light_background : The contribution to the background from stray light,\\n',\n",
       "  '                       contributing shot noise\\n',\n",
       "  '    ccd_backgrond : The contribution to the background from CCD readout,\\n',\n",
       "  '                    contributing gaussian noise\\n',\n",
       "  '    ccd_stdev : The standard deviation on the gaussian CCD noise\\n',\n",
       "  '\\n',\n",
       "  '    Optional Arguments:\\n',\n",
       "  '    -------------------\\n',\n",
       "  '    gaussian_approx: Use a gaussian approximation for the poisson distribution.\\n',\n",
       "  '                     This allows the calculation to be sped up by several\\n',\n",
       "  '                     orders of magnitude and is a good approximation as long\\n',\n",
       "  '                     as the relevant signal is greater than about ten counts.\\n',\n",
       "  '    conv_range : How many sigmas of the poisson distribution to go out to in\\n',\n",
       "  '                 the convolution. For example, if there are 1000 photons in a\\n',\n",
       "  '                 bin, the convolution is centered around 1000 and goes out to\\n',\n",
       "  '                 1000 +/- conv_range * sqrt(1000). Default value is -1, which\\n',\n",
       "  '                 goes out to 2.5 sigma. Note that the beahvior of this argument\\n',\n",
       "  \"                 changes if the 'safe' argument is True. (See below.)\\n\",\n",
       "  '    debug : if True, returns the convolution list, the convolution\\n',\n",
       "  '            matrix, the poisson matrix, the gaussian matrix, the\\n',\n",
       "  '            likelihood list, and the computed max_y. False by default,\\n',\n",
       "  '            in which case only the log-likelihood is returned.\\n',\n",
       "  '    test_norm: if True, tests to make sure that the gaussian and poisson\\n',\n",
       "  '               contributions are normalized. This is the default case.\\n',\n",
       "  '               If False, the function will run a little faster but will\\n',\n",
       "  '               not check this condition.\\n',\n",
       "  '    safe : if True, calculate the likelihood using the super-safe default\\n',\n",
       "  '           for the convolution ranges. This is much slower, but possibly\\n',\n",
       "  '           useful if you are getting unexpected results in inference.\\n',\n",
       "  '           Default: False.\\n',\n",
       "  '           This changes the behavior of the conv_range command.\\n',\n",
       "  '           If safe is True, conv_range is the total range to go out to\\n',\n",
       "  '           in the convolution (not the range in terms of sigma).\\n',\n",
       "  '           The default behavior also changes: in the default case, the\\n',\n",
       "  '           convolution goes out to min([max(y), max(model_prediction)]) plus\\n',\n",
       "  '           3 * sqrt of this max.\\n',\n",
       "  '    \"\"\"\\n',\n",
       "  '\\n',\n",
       "  '    # First, get the contribution from the light signal.\\n',\n",
       "  '    y_two_peak_model = two_peak_model(x, amp1, amp2, C0 + T * m,\\n',\n",
       "  '                                      center2, width1, width2,\\n',\n",
       "  '                                      light_background)\\n',\n",
       "  '\\n',\n",
       "  '    # If the prediction from the model is ever <= 0,\\n',\n",
       "  '    # the likelihood is zero and the log-likelihood is -np.inf\\n',\n",
       "  '    # This is because the system can never produce negative\\n',\n",
       "  '    # photons.\\n',\n",
       "  '    #\\n',\n",
       "  '    # If either background contribution is less than zero,\\n',\n",
       "  '    # the log-likelihood should also be negative infinity\\n',\n",
       "  '    # \\n',\n",
       "  '    # This is, to some extent, including a prior.\\n',\n",
       "  '\\n',\n",
       "  '    if (np.min(y_two_peak_model) <= 0 or ccd_background < 0\\n',\n",
       "  '        or light_background < 0) and not(debug):\\n',\n",
       "  '        return -np.inf\\n',\n",
       "  '\\n',\n",
       "  '    # If the gaussian approximation is being used, calculating the likelihood\\n',\n",
       "  '    # is exceedingly simple, so just do it here and skip the rest of the stuff.\\n',\n",
       "  '    # In my benchmarking, calculating this by hand is actually faster than \\n',\n",
       "  '    # using scipy.stats.\\n',\n",
       "  '    if gaussian_approx:\\n',\n",
       "  '        # Standard deviation is sum of variances.\\n',\n",
       "  '        # For poisson term, variance is mean, and mean is the model.\\n',\n",
       "  '        total_var = ccd_stdev**2 + y_two_peak_model\\n',\n",
       "  '        likelihood_list = ((1 / (np.sqrt(2.0 * np.pi * total_var))) *\\n',\n",
       "  '                         np.exp(-1.0 * (y - y_two_peak_model - ccd_background)**2 /\\n',\n",
       "  '                                (2.0 * total_var)))\\n',\n",
       "  '        return np.sum(np.log(likelihood_list))\\n',\n",
       "  '\\n',\n",
       "  '    # If safe is True, then do the naive convolution over the entire range\\n',\n",
       "  '    # of the data. This is time consuming, but should work no matter what.\\n',\n",
       "  '    # If safe is False (the default case), the bounds of the convolution will\\n',\n",
       "  '    # be adjusted for each data point. This can dramatically speed up the\\n',\n",
       "  '    # calculations (I predict 5-10x improvement) but could cause some\\n',\n",
       "  '    # undesirable consequences that I cannot forsee.\\n',\n",
       "  '    #\\n',\n",
       "  '    # To see how this works, just do a simple change of variables in the\\n',\n",
       "  '    # convolution. Instead of taking n going from yi-gamma to yi+gamma, where \\n',\n",
       "  '    # gamma is some sufficiently large number so that the probability is zero\\n',\n",
       "  '    # elsewhere, take:\\n',\n",
       "  '    # x = n - y_i\\n',\n",
       "  '    # going from - gamma to gamma.\\n',\n",
       "  '    # Mathematically, this does nothing, but it allows us to use a much smaller\\n',\n",
       "  '    # matrix for the convolution range.\\n',\n",
       "  '\\n',\n",
       "  '    if safe:\\n',\n",
       "  '        # Construct the bounds of convolution\\n',\n",
       "  '        # If the conv_range is -1, go out to the max of the\\n',\n",
       "  '        # max(y, model prediction) + 3x the square root of the max.\\n',\n",
       "  '        # etc. for min\\n',\n",
       "  '        max_y = 0\\n',\n",
       "  '        if conv_range == -1:\\n',\n",
       "  '            # Optimize these, maybe do max of min and min of max\\n',\n",
       "  '            # Before, this was not this way, which meant that size of matrices\\n',\n",
       "  '            # could grow arbitrarily depending on prediction. This way they are at\\n',\n",
       "  '            # least bounded by the actual data.\\n',\n",
       "  '            # \\n',\n",
       "  '            # The values from y should have the ccd_background subtracted, because\\n',\n",
       "  '            # the convolution goes over the physical photons, not the ccd\\n',\n",
       "  '            # background.\\n',\n",
       "  '            #\\n',\n",
       "  '            # This passes the tests, I think it is OK in inference.\\n',\n",
       "  '            min_y = np.max([np.min(y_two_peak_model), np.min(y)-ccd_background])\\n',\n",
       "  '            max_y = np.min([np.max(y_two_peak_model), np.max(y)-ccd_background])\\n',\n",
       "  \"            # Shouldn't this be np.max below? Was np.min w/ min_y in the\\n\",\n",
       "  '            # np.sqrt. Could cause some problems. I think it was related\\n',\n",
       "  '            # to not having the ccd_background above.\\n',\n",
       "  '            conv_min = np.floor(np.max([0, min_y - 3 * np.sqrt(min_y)]))\\n',\n",
       "  '            conv_max = np.floor(max_y + 3 * np.sqrt(max_y))\\n',\n",
       "  '        elif conv_range > 0:\\n',\n",
       "  '            conv_max = conv_range\\n',\n",
       "  '        else:\\n',\n",
       "  \"            raise ValueError('Range for convolution must be positive')\\n\",\n",
       "  '        \\n',\n",
       "  '        conv_list = np.arange(conv_min, conv_max)[:, np.newaxis]\\n',\n",
       "  '        # Construct the convolution matrix by broadcasting\\n',\n",
       "  '        # Probably there is a faster way to do this.\\n',\n",
       "  '        conv_mat = conv_list + 0*y\\n',\n",
       "  '        \\n',\n",
       "  '        # Construct poisson term\\n',\n",
       "  '        poisson_term = stats.poisson._pmf(conv_mat, y_two_peak_model)\\n',\n",
       "  '        # Construct gaussian term\\n',\n",
       "  '        gaussian_term = ((1 / (np.sqrt(2.0 * np.pi * ccd_stdev**2))) *\\n',\n",
       "  '                         np.exp(-1.0 * (y - conv_mat - ccd_background)**2 /\\n',\n",
       "  '                                (2.0 * ccd_stdev**2)))\\n',\n",
       "  '    else:\\n',\n",
       "  '        # else: use fast (not \"safe\" technique)\\n',\n",
       "  '\\n',\n",
       "  '        # conv_range in this case is handled a little differently\\n',\n",
       "  '        # See docstring.\\n',\n",
       "  '        if conv_range == -1:\\n',\n",
       "  '            conv_range = 2.5\\n',\n",
       "  '        elif conv_range > 0:\\n',\n",
       "  '            pass\\n',\n",
       "  '        else:\\n',\n",
       "  \"            raise ValueError('Range for convolution must be positive')\\n\",\n",
       "  '\\n',\n",
       "  '        # Construct the bounds of convolution\\n',\n",
       "  '        # floor is important here so that convolution range is only over ints.\\n',\n",
       "  '        # min_y is unnecessary for this case, but useful to pass as output of\\n',\n",
       "  '        # debug so that format is consistent.\\n',\n",
       "  '        min_y = np.max([np.min(y_two_peak_model), np.min(y)-ccd_background])\\n',\n",
       "  '        max_y = np.min([np.max(y_two_peak_model), np.max(y)-ccd_background])\\n',\n",
       "  '        conv_max = conv_range*np.sqrt(max_y)\\n',\n",
       "  '        conv_min = -1*conv_max\\n',\n",
       "  '        # Need to subtract the ccd_background here, because we are comparing\\n',\n",
       "  '        # conv_mat + y to the prediction from y_two_peak_model and the two\\n',\n",
       "  '        # differ by the ccd_background prediction. One way to look at this is\\n',\n",
       "  '        # that the convolution should go over the *physical* number of photons\\n',\n",
       "  '        # (which does not include the ccd_background) contribution, but y\\n',\n",
       "  '        # includes a contribution from the background, so we need to subtract\\n',\n",
       "  '        # the background from y, i.e. add its negative to the conv_list.\\n',\n",
       "  '        # If the background is some crazy thing, the likelihood will be low,\\n',\n",
       "  \"        # but that's OK. (It's kind of the point.)\\n\",\n",
       "  '        #\\n',\n",
       "  '        # We need to subtract it here and not e.g. in the probability\\n',\n",
       "  '        # calculation for consistency.\\n',\n",
       "  '        conv_list = np.arange(conv_min, conv_max)[:,np.newaxis]-np.floor(ccd_background)\\n',\n",
       "  '        # Construct the convolution matrix by broadcasting.\\n',\n",
       "  '        # Note that this should NOT be conv_list - y.\\n',\n",
       "  '        # That would effectively undo our change of variables and make it\\n',\n",
       "  '        # mathematically equivalent to the safe case. (I have checked this.)\\n',\n",
       "  '        conv_mat = (conv_list - 0*y)\\n',\n",
       "  \"        # Don't allow negative photons.\\n\",\n",
       "  '        # Unfortunately, this requires us to use nansum below, which I really\\n',\n",
       "  \"        # don't like.\\n\",\n",
       "  '        # This would be handled elegantly if we could use pmf instead of _pmf,\\n',\n",
       "  '        # because pmf (although a factor of two slower) will set negative\\n',\n",
       "  '        # values to zero. However, it also sets non-integer values to zero,\\n',\n",
       "  '        # and we can have non-integer values if y is non-integer valued.\\n',\n",
       "  '        #\\n',\n",
       "  '        # TODO: do change of variables such that we have an integer part of y\\n',\n",
       "  '        # and a non-integer part of y. The non-integer part shows up in the\\n',\n",
       "  '        # gaussian part, and the integer part shows up in the poisson part\\n',\n",
       "  '        # but I am not worrying about this for now.\\n',\n",
       "  '        conv_mat[conv_mat + y < 0] = np.nan\\n',\n",
       "  '        # Construct poisson term. Note change of variables for the mean.\\n',\n",
       "  '        poisson_term = stats.poisson._pmf(conv_mat + y, y_two_peak_model)\\n',\n",
       "  '        # Construct gaussian term. Change of variables means we just\\n',\n",
       "  '        # conv_mat vs. background.\\n',\n",
       "  '        gaussian_term = ((1/(np.sqrt(2.0*np.pi*ccd_stdev**2)))*\\n',\n",
       "  '                         np.exp(-1.0*(conv_mat+ccd_background)**2 / \\n',\n",
       "  '                                (2.0*ccd_stdev**2)))\\n',\n",
       "  '\\n',\n",
       "  '    # Now, in either case (safe or not safe) we have a list of likelihoods\\n',\n",
       "  '    # from the poisson and gaussian terms:\\n',\n",
       "  '\\n',\n",
       "  '    # Perform sum over zeroth axis to get likelihoods\\n',\n",
       "  '    # nansum is extremely dangerous for debugging and best avoided\\n',\n",
       "  '    likelihood_list = np.nansum(poisson_term * gaussian_term, 0)\\n',\n",
       "  '\\n',\n",
       "  '    # Test to make sure poisson and gaussian terms are normalized.\\n',\n",
       "  '    # In other words, make sure the sum in the convolution is going\\n',\n",
       "  '    # out far enough.\\n',\n",
       "  '    # I think it is OK to test only one of them, because if we are\\n',\n",
       "  '    # going completely over the support of at least one of them then\\n',\n",
       "  '    # it means that we are not missing out on any probability density\\n',\n",
       "  '    # in the convolution.\\n',\n",
       "  '    # My motivation for doing `or` instead of `and` is that the gaussian\\n',\n",
       "  '    # term seems to suffer from some discretization errors. If the\\n',\n",
       "  '    # standard deviation is small, then points can land at non-integer\\n',\n",
       "  '    # y values, which can generate a low likelihood just for the gaussian\\n',\n",
       "  '    # term.\\n',\n",
       "  '    if test_norm:\\n',\n",
       "  '        warnings.warn(\"Testing for normalization is no longer meaningful. \"+\\n',\n",
       "  '                      \"If you are trying to extract information this way, \"+\\n',\n",
       "  '                      \"you may be disappointed.\")\\n',\n",
       "  '        gauss_norm = all(np.sum(gaussian_term.T, axis = 1) > 0.99)\\n',\n",
       "  '        poiss_norm = all(np.sum(poisson_term.T, axis = 1) > 0.99)\\n',\n",
       "  '        if not(gauss_norm or poiss_norm):\\n',\n",
       "  \"            raise ValueError('The terms in the convolution are' +\\n\",\n",
       "  \"                             ' not normalized. Try increasing' +\\n\",\n",
       "  \"                             ' the conv_range variable!')\\n\",\n",
       "  '    if debug:\\n',\n",
       "  '        return [conv_list, conv_mat, poisson_term,\\n',\n",
       "  '                gaussian_term, likelihood_list, \\n',\n",
       "  '                [min_y, max_y, conv_min, conv_max], y_two_peak_model]\\n',\n",
       "  '\\n',\n",
       "  '    # Return sum of log_likelihoods\\n',\n",
       "  '    ll = np.sum(np.log(likelihood_list))\\n',\n",
       "  '    return ll\\n'],\n",
       " 30)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect.getsourcelines(sivtempfit.model.two_peak_log_likelihood)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
